{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to Data Streaming </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parse data from an RSS feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import time\n",
    "\n",
    "d = feedparser.parse('https://www.reddit.com/r/all/new/.rss')\n",
    "print(\"\")\n",
    "print(\"Pulling latest news from Reddit, sort by new. https://www.reddit.com/r/all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we pull?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making some sense out of this feed ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.get('entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many did we pull?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(d.get('entries')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who submitted what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for entry in d.get('entries'):\n",
    "    print(\"****  \" + entry.get('author') + \": \" + entry.get('title'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who submitted when?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in d.get('entries'):\n",
    "    print(\"****  \" + entry.get('author') + \": \" + entry.get('updated'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the streaming aspect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts = 5\n",
    "dt = 0.1\n",
    "\n",
    "for i in range(ts):\n",
    "    d = feedparser.parse('https://www.reddit.com/r/all/new/.rss')\n",
    "    print(\"\")\n",
    "    print(\"**** PULLING FROM REDDIT  ****\")\n",
    "    print(\"During this time step, we pull \" +  str(len(d.get('entries'))))\n",
    "    for entry in d.get('entries'):\n",
    "        print(\"****  \" + entry.get('author') + \": \" + entry.get('updated') + \": \" + entry.get('title'))\n",
    "    time.sleep(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Reduce time between pull (*dt*) and increase the number of time step (*ts*) and see whether you can extract the top-10 subreddit with the most new submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 10\n",
    "dt = 0.5\n",
    "\n",
    "subreddit = {}\n",
    "\n",
    "for i in range(ts):\n",
    "    d = feedparser.parse('https://www.reddit.com/r/all/new/.rss')\n",
    "    for entry in d.get('entries'):\n",
    "        for tag in entry.tags:\n",
    "            if tag.get('label') in subreddit:\n",
    "                if entry.get('title') != subreddit.get(tag.get('label')).get('title'):\n",
    "                    subreddit[tag.get('label')]['count'] += 1         \n",
    "                    subreddit[tag.get('label')]['title'] = entry.get('title')\n",
    "            else:\n",
    "                subreddit[tag.get('label')] = {}\n",
    "                subreddit[tag.get('label')]['count'] = 1\n",
    "                subreddit[tag.get('label')]['title'] = entry.get('title')\n",
    "    print(\"*****************\")\n",
    "    print(subreddit)\n",
    "    time.sleep(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Learn how to combine beautifulsoup4 (*bs4*) to cleanly extract reddit contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
