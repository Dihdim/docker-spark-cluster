{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '/usr/hdp/current/spark2-client/python')\n",
    "sys.path.insert(0, '/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip')\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/usr/hdp/current/spark2-client/'\n",
    "os.environ['SPARK_CONF_DIR'] = '/etc/spark2/conf'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster(\"yarn\")\n",
    "conf.set(\"spark.driver.memory\",\"2g\")\n",
    "conf.set(\"spark.executor.instances\", \"5\")\n",
    "conf.set(\"spark.executor.memory\",\"5g\")\n",
    "conf.set(\"spark.executor.cores\",\"5\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://clnode225.clemson.cloudlab.us:4053\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1.3.0.1.0-187</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Ratings\n",
    "\n",
    "An independent movie company is looking to invest in a new movie project. With limited finances, the company wants to \n",
    "analyze the reactions of audiences, particularly toward various movie genres, in order to identify a \n",
    "movie project to focus on which will help the business earn more profit. The company relies on data collected from a publicly available recommendation service by [MovieLens](http://dl.acm.org/citation.cfm?id=2827872). This [dataset](http://files.grouplens.org/datasets/movielens/ml-10m-README.html) contains **24,404,096** ratings and **668,953** tags applied across **40,110** movies. This data was created by **247,753** users between January 09, 1995 and January 29, 2016. This dataset was generated on October 17, 2016. \n",
    "\n",
    "From this dataset, several analyses are possible, include the following:\n",
    "1.   Find movies which have the highest average ratings over the years and identify the corresponding genre.\n",
    "2.   Find genres which have the highest average ratings over the years.\n",
    "3.   Find users who rate movies most frequently in order to contact them for an in-depth marketing analysis.\n",
    "\n",
    "These types of analyses, which are somewhat ambiguous, demand the ability to quickly process large amounts of data in \n",
    "a relatively short amount of time for justifying business decisions. In these situations, the size of the data typically makes analysis done on a single machine impossible and analysis done using a remote storage system impractical. For the remainder of the lessons, we will learn how HDFS provides the basis to store a massive amount of data and to enable the programming approach to analyze this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `ml-latest-small/links.csv': File exists\n",
      "put: `ml-latest-small/tags.csv': File exists\n",
      "put: `ml-latest-small/ratings.csv': File exists\n",
      "put: `ml-latest-small/README.txt': File exists\n",
      "put: `ml-latest-small/movies.csv': File exists\n",
      "Found 3 items\n",
      "drwxr-xr-x   - lngo hadoop          0 2020-01-06 15:32 .sparkStaging\n",
      "drwxr-xr-x   - lngo hadoop          0 2020-01-06 13:50 ml-latest-small\n",
      "drwxr-xr-x   - lngo hadoop          0 2020-01-02 13:27 text\n",
      "Found 5 items\n",
      "-rw-r--r--   2 lngo hadoop       8342 2020-01-06 13:50 ml-latest-small/README.txt\n",
      "-rw-r--r--   2 lngo hadoop     188236 2020-01-06 13:50 ml-latest-small/links.csv\n",
      "-rw-r--r--   2 lngo hadoop     484688 2020-01-06 13:50 ml-latest-small/movies.csv\n",
      "-rw-r--r--   2 lngo hadoop    2382886 2020-01-06 13:50 ml-latest-small/ratings.csv\n",
      "-rw-r--r--   2 lngo hadoop     114976 2020-01-06 13:50 ml-latest-small/tags.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ../data/ml-latest-small ./\n",
    "!hdfs dfs -ls\n",
    "!hdfs dfs -ls ./ml-latest-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId,title,genres',\n",
       " '1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy',\n",
       " '2,Jumanji (1995),Adventure|Children|Fantasy',\n",
       " '3,Grumpier Old Men (1995),Comedy|Romance',\n",
       " '4,Waiting to Exhale (1995),Comedy|Drama|Romance',\n",
       " '5,Father of the Bride Part II (1995),Comedy',\n",
       " '6,Heat (1995),Action|Crime|Thriller',\n",
       " '7,Sabrina (1995),Comedy|Romance',\n",
       " '8,Tom and Huck (1995),Adventure|Children',\n",
       " '9,Sudden Death (1995),Action',\n",
       " '10,GoldenEye (1995),Action|Adventure|Thriller',\n",
       " '11,\"American President, The (1995)\",Comedy|Drama|Romance',\n",
       " '12,Dracula: Dead and Loving It (1995),Comedy|Horror',\n",
       " '13,Balto (1995),Adventure|Animation|Children',\n",
       " '14,Nixon (1995),Drama',\n",
       " '15,Cutthroat Island (1995),Action|Adventure|Romance',\n",
       " '16,Casino (1995),Crime|Drama',\n",
       " '17,Sense and Sensibility (1995),Drama|Romance',\n",
       " '18,Four Rooms (1995),Comedy',\n",
       " '19,Ace Ventura: When Nature Calls (1995),Comedy',\n",
       " '20,Money Train (1995),Action|Comedy|Crime|Drama|Thriller',\n",
       " '21,Get Shorty (1995),Comedy|Crime|Thriller',\n",
       " '22,Copycat (1995),Crime|Drama|Horror|Mystery|Thriller',\n",
       " '23,Assassins (1995),Action|Crime|Thriller',\n",
       " '24,Powder (1995),Drama|Sci-Fi',\n",
       " '25,Leaving Las Vegas (1995),Drama|Romance',\n",
       " '26,Othello (1995),Drama',\n",
       " '27,Now and Then (1995),Children|Drama',\n",
       " '28,Persuasion (1995),Drama|Romance',\n",
       " '29,\"City of Lost Children, The (Cité des enfants perdus, La) (1995)\",Adventure|Drama|Fantasy|Mystery|Sci-Fi',\n",
       " '30,Shanghai Triad (Yao a yao yao dao waipo qiao) (1995),Crime|Drama',\n",
       " '31,Dangerous Minds (1995),Drama',\n",
       " '32,Twelve Monkeys (a.k.a. 12 Monkeys) (1995),Mystery|Sci-Fi|Thriller',\n",
       " '34,Babe (1995),Children|Drama',\n",
       " '36,Dead Man Walking (1995),Crime|Drama',\n",
       " '38,It Takes Two (1995),Children|Comedy',\n",
       " '39,Clueless (1995),Comedy|Romance',\n",
       " '40,\"Cry, the Beloved Country (1995)\",Drama',\n",
       " '41,Richard III (1995),Drama|War',\n",
       " '42,Dead Presidents (1995),Action|Crime|Drama',\n",
       " '43,Restoration (1995),Drama',\n",
       " '44,Mortal Kombat (1995),Action|Adventure|Fantasy',\n",
       " '45,To Die For (1995),Comedy|Drama|Thriller',\n",
       " '46,How to Make an American Quilt (1995),Drama|Romance',\n",
       " '47,Seven (a.k.a. Se7en) (1995),Mystery|Thriller',\n",
       " '48,Pocahontas (1995),Animation|Children|Drama|Musical|Romance',\n",
       " '49,When Night Is Falling (1995),Drama|Romance',\n",
       " '50,\"Usual Suspects, The (1995)\",Crime|Mystery|Thriller',\n",
       " '52,Mighty Aphrodite (1995),Comedy|Drama|Romance',\n",
       " '53,Lamerica (1994),Adventure|Drama',\n",
       " '54,\"Big Green, The (1995)\",Children|Comedy',\n",
       " '55,Georgia (1995),Drama',\n",
       " '57,Home for the Holidays (1995),Drama',\n",
       " '58,\"Postman, The (Postino, Il) (1994)\",Comedy|Drama|Romance',\n",
       " '60,\"Indian in the Cupboard, The (1995)\",Adventure|Children|Fantasy',\n",
       " '61,Eye for an Eye (1996),Drama|Thriller',\n",
       " \"62,Mr. Holland's Opus (1995),Drama\",\n",
       " \"63,Don't Be a Menace to South Central While Drinking Your Juice in the Hood (1996),Comedy|Crime\",\n",
       " '64,Two if by Sea (1996),Comedy|Romance',\n",
       " '65,Bio-Dome (1996),Comedy',\n",
       " '66,Lawnmower Man 2: Beyond Cyberspace (1996),Action|Sci-Fi|Thriller',\n",
       " '68,French Twist (Gazon maudit) (1995),Comedy|Romance',\n",
       " '69,Friday (1995),Comedy',\n",
       " '70,From Dusk Till Dawn (1996),Action|Comedy|Horror|Thriller',\n",
       " '71,Fair Game (1995),Action',\n",
       " '72,Kicking and Screaming (1995),Comedy|Drama',\n",
       " '73,\"Misérables, Les (1995)\",Drama|War',\n",
       " '74,Bed of Roses (1996),Drama|Romance',\n",
       " '75,Big Bully (1996),Comedy|Drama',\n",
       " '76,Screamers (1995),Action|Sci-Fi|Thriller',\n",
       " '77,Nico Icon (1995),Documentary',\n",
       " '78,\"Crossing Guard, The (1995)\",Action|Crime|Drama|Thriller',\n",
       " '79,\"Juror, The (1996)\",Drama|Thriller',\n",
       " '80,\"White Balloon, The (Badkonake sefid) (1995)\",Children|Drama',\n",
       " \"81,Things to Do in Denver When You're Dead (1995),Crime|Drama|Romance\",\n",
       " \"82,Antonia's Line (Antonia) (1995),Comedy|Drama\",\n",
       " '83,Once Upon a Time... When We Were Colored (1995),Drama|Romance',\n",
       " '85,Angels and Insects (1995),Drama|Romance',\n",
       " '86,White Squall (1996),Action|Adventure|Drama',\n",
       " '87,Dunston Checks In (1996),Children|Comedy',\n",
       " '88,Black Sheep (1996),Comedy',\n",
       " '89,Nick of Time (1995),Action|Thriller',\n",
       " '92,Mary Reilly (1996),Drama|Horror|Thriller',\n",
       " '93,Vampire in Brooklyn (1995),Comedy|Horror|Romance',\n",
       " '94,Beautiful Girls (1996),Comedy|Drama|Romance',\n",
       " '95,Broken Arrow (1996),Action|Adventure|Thriller',\n",
       " '96,In the Bleak Midwinter (1995),Comedy|Drama',\n",
       " '97,\"Hate (Haine, La) (1995)\",Crime|Drama',\n",
       " '99,Heidi Fleiss: Hollywood Madam (1995),Documentary',\n",
       " '100,City Hall (1996),Drama|Thriller',\n",
       " '101,Bottle Rocket (1996),Adventure|Comedy|Crime|Romance',\n",
       " '102,Mr. Wrong (1996),Comedy',\n",
       " '103,Unforgettable (1996),Mystery|Sci-Fi|Thriller',\n",
       " '104,Happy Gilmore (1996),Comedy',\n",
       " '105,\"Bridges of Madison County, The (1995)\",Drama|Romance',\n",
       " '106,Nobody Loves Me (Keiner liebt mich) (1994),Comedy|Drama',\n",
       " '107,Muppet Treasure Island (1996),Adventure|Children|Comedy|Musical',\n",
       " '108,Catwalk (1996),Documentary',\n",
       " '110,Braveheart (1995),Action|Drama|War',\n",
       " '111,Taxi Driver (1976),Crime|Drama|Thriller']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!hdfs dfs -cat ml-latest-small/movies.csv \\\n",
    "    2>/dev/null | head -n 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId,movieId,rating,timestamp',\n",
       " '1,1,4.0,964982703',\n",
       " '1,3,4.0,964981247',\n",
       " '1,6,4.0,964982224',\n",
       " '1,47,5.0,964983815',\n",
       " '1,50,5.0,964982931',\n",
       " '1,70,3.0,964982400',\n",
       " '1,101,5.0,964980868',\n",
       " '1,110,4.0,964982176',\n",
       " '1,151,5.0,964984041',\n",
       " '1,157,5.0,964984100',\n",
       " '1,163,5.0,964983650',\n",
       " '1,216,5.0,964981208',\n",
       " '1,223,3.0,964980985',\n",
       " '1,231,5.0,964981179',\n",
       " '1,235,4.0,964980908',\n",
       " '1,260,5.0,964981680',\n",
       " '1,296,3.0,964982967',\n",
       " '1,316,3.0,964982310',\n",
       " '1,333,5.0,964981179',\n",
       " '1,349,4.0,964982563',\n",
       " '1,356,4.0,964980962',\n",
       " '1,362,5.0,964982588',\n",
       " '1,367,4.0,964981710',\n",
       " '1,423,3.0,964982363',\n",
       " '1,441,4.0,964980868',\n",
       " '1,457,5.0,964981909',\n",
       " '1,480,4.0,964982346',\n",
       " '1,500,3.0,964981208',\n",
       " '1,527,5.0,964984002',\n",
       " '1,543,4.0,964981179',\n",
       " '1,552,4.0,964982653',\n",
       " '1,553,5.0,964984153',\n",
       " '1,590,4.0,964982546',\n",
       " '1,592,4.0,964982271',\n",
       " '1,593,4.0,964983793',\n",
       " '1,596,5.0,964982838',\n",
       " '1,608,5.0,964982931',\n",
       " '1,648,3.0,964982563',\n",
       " '1,661,5.0,964982838',\n",
       " '1,673,3.0,964981775',\n",
       " '1,733,4.0,964982400',\n",
       " '1,736,3.0,964982653',\n",
       " '1,780,3.0,964984086',\n",
       " '1,804,4.0,964980499',\n",
       " '1,919,5.0,964982475',\n",
       " '1,923,5.0,964981529',\n",
       " '1,940,5.0,964982176',\n",
       " '1,943,4.0,964983614',\n",
       " '1,954,5.0,964983219',\n",
       " '1,1009,3.0,964981775',\n",
       " '1,1023,5.0,964982681',\n",
       " '1,1024,5.0,964982876',\n",
       " '1,1025,5.0,964982791',\n",
       " '1,1029,5.0,964982855',\n",
       " '1,1030,3.0,964982903',\n",
       " '1,1031,5.0,964982653',\n",
       " '1,1032,5.0,964982791',\n",
       " '1,1042,4.0,964981179',\n",
       " '1,1049,5.0,964982400',\n",
       " '1,1060,4.0,964980924',\n",
       " '1,1073,5.0,964981680',\n",
       " '1,1080,5.0,964981327',\n",
       " '1,1089,5.0,964982951',\n",
       " '1,1090,4.0,964984018',\n",
       " '1,1092,5.0,964983484',\n",
       " '1,1097,5.0,964981680',\n",
       " '1,1127,4.0,964982513',\n",
       " '1,1136,5.0,964981327',\n",
       " '1,1196,5.0,964981827',\n",
       " '1,1197,5.0,964981872',\n",
       " '1,1198,5.0,964981827',\n",
       " '1,1206,5.0,964983737',\n",
       " '1,1208,4.0,964983250',\n",
       " '1,1210,5.0,964980499',\n",
       " '1,1213,5.0,964982951',\n",
       " '1,1214,4.0,964981855',\n",
       " '1,1219,2.0,964983393',\n",
       " '1,1220,5.0,964981909',\n",
       " '1,1222,5.0,964981909',\n",
       " '1,1224,5.0,964984018',\n",
       " '1,1226,5.0,964983618',\n",
       " '1,1240,5.0,964983723',\n",
       " '1,1256,5.0,964981442',\n",
       " '1,1258,3.0,964983414',\n",
       " '1,1265,4.0,964983599',\n",
       " '1,1270,5.0,964983705',\n",
       " '1,1275,5.0,964982290',\n",
       " '1,1278,5.0,964983414',\n",
       " '1,1282,5.0,964982703',\n",
       " '1,1291,5.0,964981909',\n",
       " '1,1298,5.0,964984086',\n",
       " '1,1348,4.0,964983393',\n",
       " '1,1377,3.0,964982653',\n",
       " '1,1396,3.0,964983017',\n",
       " '1,1408,3.0,964982310',\n",
       " '1,1445,3.0,964984112',\n",
       " '1,1473,4.0,964980875',\n",
       " '1,1500,4.0,964980985',\n",
       " '1,1517,5.0,964981107']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!hdfs dfs -cat ml-latest-small/ratings.csv \\\n",
    "    2>/dev/null | head -n 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f9bdc1191d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = pyspark.SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ratings = sc.textFile(\"./ml-latest-small/ratings.csv\")\n",
    "\n",
    "ratings = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .load(\"./ml-latest-small/ratings.csv\")\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 ms, sys: 185 µs, total: 1.3 ms\n",
      "Wall time: 1.07 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 818 µs, sys: 985 µs, total: 1.8 ms\n",
      "Wall time: 145 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ratings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Find movies which have the highest average ratings over the years and identify the corresponding genre.\n",
    "\n",
    "- Find the average ratings of all movies over the years\n",
    "- Identify the corresponding genres for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=1, movieId=1, rating=4.0, timestamp=964982703),\n",
       " Row(userId=1, movieId=3, rating=4.0, timestamp=964981247),\n",
       " Row(userId=1, movieId=6, rating=4.0, timestamp=964982224),\n",
       " Row(userId=1, movieId=47, rating=5.0, timestamp=964983815),\n",
       " Row(userId=1, movieId=50, rating=5.0, timestamp=964982931)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.registerTempTable(\"ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|movieId|AvgRating|\n",
      "+-------+---------+\n",
      "| 135216|      0.5|\n",
      "|  54768|      0.5|\n",
      "| 122627|      0.5|\n",
      "| 107013|      0.5|\n",
      "|   4775|      0.5|\n",
      "|  67799|      0.5|\n",
      "|  85334|      0.5|\n",
      "|  57326|      0.5|\n",
      "|   7742|      0.5|\n",
      "|  31422|      0.5|\n",
      "|  92681|      0.5|\n",
      "|  60363|      0.5|\n",
      "|  89386|      0.5|\n",
      "|   5771|      0.5|\n",
      "| 122888|      0.5|\n",
      "| 134246|      0.5|\n",
      "|  72424|      0.5|\n",
      "|   5105|      0.5|\n",
      "| 109897|      0.5|\n",
      "| 138186|      0.5|\n",
      "+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 2 ms, sys: 3.38 ms, total: 5.38 ms\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avgRatings = sqlContext.sql(\"SELECT movieId, AVG(rating) AS AvgRating \\\n",
    "                                    FROM ratings \\\n",
    "                                    GROUP BY movieId \\\n",
    "                                    ORDER BY AvgRating\")\n",
    "avgRatings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we augment movie ratings data with title information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movies = sc.textFile(\"./ml-latest-small/movies.csv\")\n",
    "movies = sqlContext.read.format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferschema\", \"true\")\\\n",
    "    .load(\"./ml-latest-small/movies.csv\")\\\n",
    "    .cache()\n",
    "movies.registerTempTable(\"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----------+\n",
      "|               title|movieId|AvgRatings|\n",
      "+--------------------+-------+----------+\n",
      "|    Lady Jane (1986)|   6201|       5.0|\n",
      "|Runaway Brain (19...|  96608|       5.0|\n",
      "|Awfully Big Adven...|    148|       5.0|\n",
      "|  Tokyo Tribe (2014)| 138632|       5.0|\n",
      "|Karlson Returns (...| 172585|       5.0|\n",
      "|Martin Lawrence L...|   5513|       5.0|\n",
      "|    All Yours (2016)| 158882|       5.0|\n",
      "|Entertaining Ange...|   1140|       5.0|\n",
      "|Gena the Crocodil...| 175293|       5.0|\n",
      "|Slumber Party Mas...|   3940|       5.0|\n",
      "|Dr. Goldfoot and ...|   4402|       5.0|\n",
      "|20 Million Miles ...|   5468|       5.0|\n",
      "|   The Editor (2015)| 142444|       5.0|\n",
      "|Trinity and Sarta...| 128087|       5.0|\n",
      "|Crippled Avengers...| 115727|       5.0|\n",
      "|Enter the Void (2...|  78836|       5.0|\n",
      "|Mickey's Once Upo...|  72692|       5.0|\n",
      "|      My Love (2006)| 134095|       5.0|\n",
      "|Go for Zucker! (A...|  44851|       5.0|\n",
      "| On the Ropes (1999)|   2824|       5.0|\n",
      "+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 2.86 ms, sys: 561 µs, total: 3.42 ms\n",
      "Wall time: 769 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "avgRatings = sqlContext.sql(\"SELECT m.title, r.movieId, AVG(r.rating) AS AvgRatings \\\n",
    "                                    FROM ratings AS r \\\n",
    "                                    INNER JOIN movies AS m \\\n",
    "                                    ON m.movieId = r.movieId \\\n",
    "                                    GROUP BY r.movieId, m.title \\\n",
    "                                    ORDER BY AvgRatings DESC\")\n",
    "avgRatings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "Make appropriate changes so that only movies with average ratings higher than 3.75 and number of ratings totalling at least 1000 are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+----------+-----+\n",
      "| first(title, false)|first(movieId, false)|AvgRatings|Count|\n",
      "+--------------------+---------------------+----------+-----+\n",
      "|     Lamerica (1994)|                   53|       5.0|    2|\n",
      "|Adventures Of She...|               147300|       5.0|    1|\n",
      "|Supercop 2 (Proje...|                  876|       5.0|    1|\n",
      "|  Palindromes (2004)|                33138|       5.0|    1|\n",
      "|Passenger, The (P...|                26350|       5.0|    1|\n",
      "|Strictly Sexual (...|                67618|       5.0|    1|\n",
      "|          61* (2001)|                27373|       5.0|    1|\n",
      "| The Love Bug (1997)|               150554|       5.0|    1|\n",
      "|Tales of Manhatta...|                25887|       5.0|    1|\n",
      "|Denise Calls Up (...|                  633|       5.0|    1|\n",
      "|Zeitgeist: Moving...|                84273|       5.0|    1|\n",
      "|Awfully Big Adven...|                  148|       5.0|    1|\n",
      "|   The Editor (2015)|               142444|       5.0|    1|\n",
      "|Raise Your Voice ...|                 8911|       5.0|    1|\n",
      "|One I Love, The (...|               113829|       5.0|    1|\n",
      "|What Happened Was...|                  496|       5.0|    1|\n",
      "|Who Killed Chea V...|               152711|       5.0|    1|\n",
      "|Martin Lawrence L...|                 5513|       5.0|    1|\n",
      "|      Empties (2007)|               173963|       5.0|    1|\n",
      "|Tenchi Muyô! In L...|               157775|       5.0|    1|\n",
      "+--------------------+---------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 3.05 ms, sys: 0 ns, total: 3.05 ms\n",
      "Wall time: 851 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ratingData = sqlContext.sql(\"SELECT FIRST(m.title), FIRST(r.movieId), AVG(r.rating) AS AvgRatings, COUNT(r.movieId) as Count \\\n",
    "                                    FROM ratings AS r \\\n",
    "                                    INNER JOIN movies AS m \\\n",
    "                                    ON m.movieId = r.movieId \\\n",
    "                                    GROUP BY r.movieId \\\n",
    "                                    ORDER BY AvgRatings DESC\")\n",
    "ratingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+------------------+-----+\n",
      "| first(title, false)|first(movieId, false)|        AvgRatings|Count|\n",
      "+--------------------+---------------------+------------------+-----+\n",
      "|Shawshank Redempt...|                  318| 4.429022082018927|  317|\n",
      "|Godfather, The (1...|                  858|         4.2890625|  192|\n",
      "|   Fight Club (1999)|                 2959| 4.272935779816514|  218|\n",
      "|Cool Hand Luke (1...|                 1276| 4.271929824561403|   57|\n",
      "|Dr. Strangelove o...|                  750| 4.268041237113402|   97|\n",
      "|  Rear Window (1954)|                  904| 4.261904761904762|   84|\n",
      "|Godfather: Part I...|                 1221|  4.25968992248062|  129|\n",
      "|Departed, The (2006)|                48516| 4.252336448598131|  107|\n",
      "|   Goodfellas (1990)|                 1213|              4.25|  126|\n",
      "|   Casablanca (1942)|                  912|              4.24|  100|\n",
      "|Dark Knight, The ...|                58559| 4.238255033557047|  149|\n",
      "|Usual Suspects, T...|                   50| 4.237745098039215|  204|\n",
      "|Princess Bride, T...|                 1197| 4.232394366197183|  142|\n",
      "|Star Wars: Episod...|                  260| 4.231075697211155|  251|\n",
      "|Schindler's List ...|                  527|             4.225|  220|\n",
      "|Apocalypse Now (1...|                 1208| 4.219626168224299|  107|\n",
      "|American History ...|                 2329| 4.217054263565892|  129|\n",
      "|Star Wars: Episod...|                 1196|4.2156398104265405|  211|\n",
      "|    Chinatown (1974)|                 1252| 4.211864406779661|   59|\n",
      "|Raiders of the Lo...|                 1198|            4.2075|  200|\n",
      "+--------------------+---------------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingData.registerTempTable(\"ratingData\")\n",
    "selectedRating = sqlContext.sql(\"SELECT * FROM ratingData \\\n",
    "                                    WHERE AvgRatings > 3.5 and COUNT > 50 \\\n",
    "                                    ORDER BY AvgRatings DESC\")\n",
    "selectedRating.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Find genres which have the highest average ratings over the years\n",
    "\n",
    "- Identify the genres associated with a movie and its rating\n",
    "- Each movie can have multiple genres. How to flip the Key/Value pair?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+-----+\n",
      "|movieId|              genres|SumRatings|Count|\n",
      "+-------+--------------------+----------+-----+\n",
      "|    148|               Drama|       5.0|    1|\n",
      "|    471|              Comedy|     142.0|   40|\n",
      "|    496|Comedy|Drama|Roma...|       5.0|    1|\n",
      "|    833|              Comedy|      12.0|    6|\n",
      "|   1088|Drama|Musical|Rom...|     141.5|   42|\n",
      "|   1238|              Comedy|      36.5|    9|\n",
      "|   1342|     Horror|Thriller|      27.5|   11|\n",
      "|   1580|Action|Comedy|Sci-Fi|     575.5|  165|\n",
      "|   1591|Action|Adventure|...|      68.5|   26|\n",
      "|   1645|Drama|Mystery|Thr...|     174.0|   51|\n",
      "|   1829|       Drama|Romance|       6.5|    2|\n",
      "|   1959|       Drama|Romance|      55.0|   15|\n",
      "|   2122|     Horror|Thriller|      27.0|   11|\n",
      "|   2142|Adventure|Animati...|      27.0|   10|\n",
      "|   2366|Action|Adventure|...|      91.0|   25|\n",
      "|   2659|  Comedy|Documentary|       2.0|    1|\n",
      "|   2866|               Drama|      18.0|    5|\n",
      "|   3175|Adventure|Comedy|...|     268.5|   75|\n",
      "|   3794|        Comedy|Drama|       5.0|    2|\n",
      "|   3918|              Horror|      29.5|    9|\n",
      "+-------+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 1.03 ms, sys: 2.29 ms, total: 3.32 ms\n",
      "Wall time: 475 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "allData = sqlContext.sql(\"SELECT FIRST(r.movieId) as movieId, FIRST(m.genres) as genres, SUM(r.rating) AS SumRatings, COUNT(r.movieId) as Count \\\n",
    "                                    FROM ratings AS r \\\n",
    "                                    INNER JOIN movies AS m \\\n",
    "                                    ON m.movieId = r.movieId \\\n",
    "                                    GROUP BY r.movieId\")\n",
    "allData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.split_tab(s)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_tab(s):\n",
    "    return s.split(\"|\")\n",
    "sqlContext.udf.register(\"split_tab\", split_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Action', 'Adventure', 'Comedy']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tab(\"Action|Adventure|Comedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'explode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-2b6f2d283cbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1182\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'explode'"
     ]
    }
   ],
   "source": [
    "allData.explode(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o503.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 101.0 failed 4 times, most recent failure: Lost task 0.3 in stage 101.0 (TID 3991, clnode229.clemson.cloudlab.us, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 235, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 230, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-66-7d75bcdc7ac0>\", line 2, in split_tab\nNameError: name 'array' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1597)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1596)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1596)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1830)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 235, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 230, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-66-7d75bcdc7ac0>\", line 2, in split_tab\nNameError: name 'array' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-592b861a4c34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mallData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allData\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgenreData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT split_tab(genres), SumRatings, Count FROM allData'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenreData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o503.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 101.0 failed 4 times, most recent failure: Lost task 0.3 in stage 101.0 (TID 3991, clnode229.clemson.cloudlab.us, executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 235, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 230, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-66-7d75bcdc7ac0>\", line 2, in split_tab\nNameError: name 'array' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1609)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1597)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1596)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1596)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1830)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1779)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1768)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.GeneratedMethodAccessor78.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 235, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 230, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 331, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 140, in dump_stream\n    for obj in iterator:\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/serializers.py\", line 320, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/worker.py\", line 76, in <lambda>\n    return lambda *a: f(*a)\n  File \"/hadoop/yarn/local/usercache/lngo/appcache/application_1578081381073_0140/container_e01_1578081381073_0140_01_000005/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-66-7d75bcdc7ac0>\", line 2, in split_tab\nNameError: name 'array' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#spark.range(1, 20).registerTempTable(\"test\")\n",
    "allData.registerTempTable(\"allData\")\n",
    "genreData = sqlContext.sql('SELECT split_tab(genres), SumRatings, Count FROM allData')\n",
    "genreData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|                 col|SumRatings|Count|\n",
      "+--------------------+----------+-----+\n",
      "|             [Drama]|       5.0|    1|\n",
      "|            [Comedy]|     142.0|   40|\n",
      "|[Comedy, Drama, R...|       5.0|    1|\n",
      "|            [Comedy]|      12.0|    6|\n",
      "|[Drama, Musical, ...|     141.5|   42|\n",
      "|            [Comedy]|      36.5|    9|\n",
      "|  [Horror, Thriller]|      27.5|   11|\n",
      "|[Action, Comedy, ...|     575.5|  165|\n",
      "|[Action, Adventur...|      68.5|   26|\n",
      "|[Drama, Mystery, ...|     174.0|   51|\n",
      "|    [Drama, Romance]|       6.5|    2|\n",
      "|    [Drama, Romance]|      55.0|   15|\n",
      "|  [Horror, Thriller]|      27.0|   11|\n",
      "|[Adventure, Anima...|      27.0|   10|\n",
      "|[Action, Adventur...|      91.0|   25|\n",
      "|[Comedy, Document...|       2.0|    1|\n",
      "|             [Drama]|      18.0|    5|\n",
      "|[Adventure, Comed...|     268.5|   75|\n",
      "|     [Comedy, Drama]|       5.0|    2|\n",
      "|            [Horror]|      29.5|    9|\n",
      "+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genreData = sqlContext.sql('SELECT explode(array(split_tab(genres))), SumRatings, Count FROM allData')\n",
    "genreData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('50', (5.0, ('\"Usual Suspects, The (1995)\"', 'Crime|Mystery|Thriller'))),\n",
       " ('50', (4.0, ('\"Usual Suspects, The (1995)\"', 'Crime|Mystery|Thriller'))),\n",
       " ('50', (1.0, ('\"Usual Suspects, The (1995)\"', 'Crime|Mystery|Thriller'))),\n",
       " ('50', (4.5, ('\"Usual Suspects, The (1995)\"', 'Crime|Mystery|Thriller'))),\n",
       " ('50', (5.0, ('\"Usual Suspects, The (1995)\"', 'Crime|Mystery|Thriller')))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentedInfo.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Adventure', 3.0), ('Animation', 3.0), ('Children', 3.0), ('Comedy', 3.0), ('Fantasy', 3.0)]\n"
     ]
    }
   ],
   "source": [
    "def extractGenreRating (t):\n",
    "    final_tuples = []\n",
    "    genreList = t[1][1][1].split(\"|\")\n",
    "    for genre in genreList:\n",
    "        final_tuples.append((genre,t[1][0]))\n",
    "    return final_tuples\n",
    "print(extractGenreRating((u'1', (3.0, (u'Toy Story (1995)', u'Adventure|Animation|Children|Comedy|Fantasy')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "genreRatings = augmentedInfo.flatMap(extractGenreRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Adventure': 24161,\n",
       "             'Fantasy': 11834,\n",
       "             'IMAX': 4145,\n",
       "             'Comedy': 39053,\n",
       "             'Action': 30635,\n",
       "             'Sci-Fi': 17243,\n",
       "             'Romance': 18124,\n",
       "             'Animation': 6988,\n",
       "             'Children': 9208,\n",
       "             'Drama': 41928,\n",
       "             'Thriller': 26452,\n",
       "             'Crime': 16681,\n",
       "             'War': 4859,\n",
       "             'Mystery': 7674,\n",
       "             'Western': 1930,\n",
       "             'Musical': 4138,\n",
       "             'Horror': 7291,\n",
       "             'Film-Noir': 870,\n",
       "             'Documentary': 1219,\n",
       "             '(no genres listed)': 47})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countsByKey = genreRatings.countByKey()\n",
    "countsByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "Complete the remainder of the steps to find the average rating of each genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
